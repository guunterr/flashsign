{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os, torch\n",
    "%load_ext wurlitzer\n",
    "from importlib import reload\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import kernels.flashattn16_128\n",
    "reload(kernels.flashattn16_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "qbar = 23040\n",
    "xbar = 8192\n",
    "DIM = 128\n",
    "big_randomQ = torch.randn(qbar, DIM, device='cuda', dtype=torch.half)\n",
    "big_randomK = torch.randn(xbar, DIM, device='cuda', dtype=torch.half)\n",
    "#big_randomV = torch.randn(xbar, DIM, device='cuda', dtype=torch.half)\n",
    "big_randomV = torch.ones(xbar, DIM, device='cuda', dtype=torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "data_size = 2*(2*big_randomQ.numel() + big_randomK.numel() + big_randomV.numel())\n",
    "\n",
    "class Moments:\n",
    "    def __init__(self, m1=0.0, m2=0.0, n=0):\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.n = n\n",
    "    def add(self, v):\n",
    "        self.m1 = (self.m1*self.n + v)/(self.n+1)\n",
    "        self.m2 = (self.m2*self.n + v**2)/(self.n+1)\n",
    "        self.n += 1\n",
    "    def std(self):\n",
    "        return self.m1, math.sqrt(self.m2-self.m1**2)\n",
    "    def __str__(self):\n",
    "        return str_std(*self.std())\n",
    "\n",
    "def str_std(m, s, additional=1):\n",
    "    precision_m = math.floor(math.log10(m))\n",
    "    precision_s = math.floor(math.log10(s))\n",
    "    precision_o = precision_m - precision_s\n",
    "    return f\"{m:.2e}Â±{s:.2e}\"\n",
    "\n",
    "def benchmarkFlash(inputLambda, reps=1):\n",
    "    # Mean for harmonics\n",
    "    mn = Moments()\n",
    "    t0 = time.time()\n",
    "    for _ in range(reps):\n",
    "        s0 = time.time()\n",
    "        input, O, mz = inputLambda()\n",
    "        torch.cuda.synchronize()\n",
    "        mn.add(1/(time.time()-s0))\n",
    "    t1 = time.time()\n",
    "    ht, sht = mn.std()\n",
    "    d = input[1].item()\n",
    "    b = input[2].item()\n",
    "    r = reps\n",
    "    # Read Rate\n",
    "    T0 = (d*ht) / (1024**3)\n",
    "    Ts = (d*sht) / (1024**3)\n",
    "    # Deposit Rate\n",
    "    T1 = (d*b*ht) / (1024**3)\n",
    "    sT1 = (d*b*sht) / (1024**3)\n",
    "    print(f\"Throughput: {str_std(T0,Ts)} GiB/s in -> {T1:.2e} GiB/s out ({int(b)}bl, {t1-t0:.2e}s)\")\n",
    "    return input, O, mz\n",
    "\n",
    "iv, O, mz = benchmarkFlash(lambda: kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV), 50)\n",
    "#iv, O, mz = benchmarkFlash(lambda: kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(mz)\n",
    "with open(\"text.txt\",\"r\") as f:\n",
    "    list_of = [float(r) for r in f.readlines()]\n",
    "\n",
    "list_of = list_of[:4096]\n",
    "sum(list_of)\n",
    "list_of\n",
    "print(len(list_of))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "QK = (big_randomQ @ big_randomK.transpose(0,1))/math.sqrt(DIM)\n",
    "list_of3 = []\n",
    "import math\n",
    "from numpy import float16, float32\n",
    "\n",
    "def process_like(xs):\n",
    "    max_counter = 0\n",
    "    for i, y in enumerate(xs):\n",
    "        if (i==0):\n",
    "            m = float16(y)\n",
    "            z = 1\n",
    "        elif (y < m):\n",
    "            y = float16(math.exp(float16(float16(y)-m)))\n",
    "            z += float32(y)\n",
    "        else:\n",
    "            max_counter+=1\n",
    "            mult = float16(math.exp(float16(m-float16(y))))\n",
    "            m = float16(y)\n",
    "            z = float32(mult*z + 1)\n",
    "        list_of3.append(z)\n",
    "    return m, z, max_counter\n",
    "process_like([x.item() for x in QK[0,:]])\n",
    "print(len(list_of3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch, math\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import float16, float32\n",
    "# Generate the random variables\n",
    "qbar = 2048\n",
    "xbar = 4096*2 # This matches 2*the window size of Mixtral\n",
    "DIM = 128   # This matches the head-dim of Mixtral\n",
    "big_randomQ = torch.randn(qbar, DIM, device='cpu', dtype=torch.half)\n",
    "big_randomK = torch.randn(xbar, DIM, device='cpu', dtype=torch.half)\n",
    "# QK matrix, [qbar, xbar]\n",
    "QK = (big_randomQ @ big_randomK.transpose(0,1))/math.sqrt(DIM)\n",
    "\n",
    "current_l = []\n",
    "\n",
    "# Process in f32\n",
    "def process_f32(xs):\n",
    "    list_l = []\n",
    "    for i, y in enumerate(xs):\n",
    "        if (i==0):\n",
    "            m = y\n",
    "            l = 1\n",
    "        elif (y < m):\n",
    "            y = math.exp(y-m)\n",
    "            l += y\n",
    "        else:\n",
    "            mult = math.exp(m-y)\n",
    "            m = y\n",
    "            l = mult*l + 1\n",
    "        list_l.append(l)\n",
    "    return list_l\n",
    "\n",
    "# Accumulate in f32, but calculate in f16\n",
    "def process_mixed(xs):\n",
    "    list_l = []\n",
    "    for i, y in enumerate(xs):\n",
    "        if (i==0):\n",
    "            m = float16(y)\n",
    "            l = 1\n",
    "        elif (y < m):\n",
    "            y = float16(math.exp(y-m))\n",
    "            l += y\n",
    "        else:\n",
    "            mult = float16(math.exp(m-y))\n",
    "            m = float16(y)\n",
    "            l = mult*l + 1\n",
    "        list_l.append(l)\n",
    "    return list_l\n",
    "\n",
    "# Process in f16\n",
    "def process_f16(xs):\n",
    "    list_l = []\n",
    "    for i, y in enumerate(xs):\n",
    "        if (i==0):\n",
    "            m = float16(y)\n",
    "            l = 1\n",
    "        elif (y < m):\n",
    "            y = float16(math.exp(y-m))\n",
    "            l += float16(y)\n",
    "        else:\n",
    "            mult = float16(math.exp(m-y))\n",
    "            m = float16(y)\n",
    "            l = float16(mult*l + 1)\n",
    "        list_l.append(l)\n",
    "    return list_l\n",
    "\n",
    "# First Plot - Accumulated Values\n",
    "for q_index in range(1):\n",
    "    plt.plot(process_f16  (QK[q_index,:])   , c='r', linestyle=\"-.\", label=\"FP16\") \n",
    "    plt.plot(process_mixed(QK[q_index,:]) , c='y', linestyle=\"solid\", label=\"FP16, FP32 $l$\")\n",
    "    plt.plot(process_f32  (QK[q_index,:])   , c='g', linestyle=\"-.\", label=\"FP32\")\n",
    "    plt.plot([4096,4096],[0,500], label=\"Mixtral Window Size (4096)\")\n",
    "plt.title(\"Accumulated $l$ Value for Various Precisions\")\n",
    "plt.xlabel(\"Progress Along $\\overline{x}$ Axis\")\n",
    "plt.ylabel(\"Accumulated $l$ Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Second Plot - Ratios\n",
    "for q_index in range(12):\n",
    "    plt.plot([x/y for x,y in zip(process_f16(QK[q_index,:]), process_f32(QK[q_index,:]))])\n",
    "plt.plot([4096,4096],[0.65,1], label=\"Mixtral Window Size (4096)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "torch.float16(2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(list_of, c='b')\n",
    "plt.plot(list_of2, c='r')\n",
    "plt.plot(list_of3, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(list_of2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(mz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "QK = (big_randomQ @ big_randomK.transpose(0,1))/math.sqrt(DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(QK)\n",
    "print(QK.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "QK[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(QK.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "m = QK.max(axis=1)[0]\n",
    "print(m.shape)\n",
    "print(torch.exp(QK-m.unsqueeze(1)).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ys=torch.tensor([-0.951660,\n",
    "-0.529785,\n",
    "-1.196289,\n",
    "-1.516602,\n",
    "-1.274414,\n",
    "1.180664,\n",
    "-0.781738,\n",
    "-0.147705,\n",
    "-0.207153,\n",
    "-0.065247,\n",
    "0.023468,\n",
    "1.379883,\n",
    "0.393555,\n",
    "0.532227,\n",
    "-0.342529,\n",
    "0.837402,\n",
    "0.633301,\n",
    "0.030380,\n",
    "-1.551758,\n",
    "1.186523,\n",
    "0.636230,\n",
    "0.350098,\n",
    "-0.115295,\n",
    "-1.531250],dtype=torch.float16)\n",
    "torch.exp(ys-torch.max(ys)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import flash_attn\n",
    "def benchmarkBase(inputLambda, reps=1, data_size=data_size):\n",
    "    Qs = big_randomQ.reshape(1, qbar, 1, DIM)\n",
    "    Ks = big_randomK.reshape(1, xbar, 1, DIM)\n",
    "    Vs = big_randomV.reshape(1, xbar, 1, DIM)\n",
    "    mn = Moments()\n",
    "    t0 = time.time()\n",
    "    for _ in range(reps):\n",
    "        s0 = time.time()\n",
    "        Onew = inputLambda(Qs, Ks, Vs)\n",
    "        torch.cuda.synchronize()\n",
    "        mn.add(1/(time.time()-s0))\n",
    "    t1 = time.time()\n",
    "    ht, sht = mn.std()\n",
    "    # Read/Write Rate\n",
    "    T0 = (data_size*ht) / (1024**3)\n",
    "    Ts = (data_size*sht)/ (1024**3)\n",
    "    print(f\"Base Throughput: {str_std(T0,Ts)} GiB/s in ({t1-t0:.2e}s)\")\n",
    "    return Onew\n",
    "\n",
    "Obase = benchmarkBase(flash_attn.flash_attn_func, 100)\n",
    "len(Obase.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(O)\n",
    "print(Obase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(O.shape)\n",
    "print(Obase.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "delta = Obase.reshape(qbar,DIM)-O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Alpha = np.float32(3.14)\n",
    "Beta = Alpha.tobytes()[1::2]\n",
    "float16 = np.frombuffer(np.frombuffer(Beta, dtype='u1'), dtype='f2')\n",
    "print(Alpha,float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "torch.max(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer\n",
    "from importlib import reload\n",
    "import os\n",
    "import kernels.benchmarks\n",
    "reload(kernels.benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer\n",
    "from importlib import reload\n",
    "import kernels.flashattn32_128\n",
    "reload(kernels.flashattn32_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qbar = 4096\n",
    "xbar = 9060*8\n",
    "DIM = 128\n",
    "big_randomQ = torch.rand(qbar, DIM, device='cuda')\n",
    "big_randomK = torch.rand(xbar, DIM, device='cuda')\n",
    "big_randomV = torch.rand(xbar, DIM, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarkFlash(inputVector):\n",
    "    input, O = inputVector\n",
    "    t = input[0].item() / 1000\n",
    "    d = input[1].item()\n",
    "    b = input[2].item()\n",
    "    r = input[3].item()\n",
    "    # Read Rate\n",
    "    T0 = r*(d/t) / (1024**3)\n",
    "    # Deposit Rate\n",
    "    T1 = r*(d*b/t) / (1024**3)\n",
    "    print(f\"Throughput: {T0:.2e} GiB/s in -> {T1:.2e} GiB/s out ({b:.0}bl, {t:.2e}s)\")\n",
    "\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 8,  8, 1))\n",
    "\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 1,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 2,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 4,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 8,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 12, 12,1))\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 8, 8, 8,1))\n",
    "benchmarkFlash(kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 4, 4, 4,1))\n",
    "\n",
    "#benchmarkFlash(kernels.benchmarks.loadFlash(big_randomQ, big_randomK, big_randomV, 32, 32, 8, 1280))\n",
    "\n",
    "vect, O = kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 1,  8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer\n",
    "from importlib import reload\n",
    "import kernels.flashattn16_128\n",
    "reload(kernels.flashattn16_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarkFlash(inputVector):\n",
    "    input, O = inputVector\n",
    "    t = input[0].item() / 1000\n",
    "    d = input[1].item()\n",
    "    b = input[2].item()\n",
    "    r = input[3].item()\n",
    "    # Read Rate\n",
    "    T0 = r*(d/t) / (1024**3)\n",
    "    # Deposit Rate\n",
    "    T1 = r*(d*b/t) / (1024**3)\n",
    "    print(f\"Throughput: {T0:.2e} GiB/s in -> {T1:.2e} GiB/s out ({b:.0}bl, {t:.2e}s)\")\n",
    "\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 8,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 1,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 2,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 4,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 8,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 12, 12,1))\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 8,  8,  8, 1))\n",
    "benchmarkFlash(kernels.flashattn16_128.flashAttention(big_randomQ, big_randomK, big_randomV, 4,  4,  4, 1))\n",
    "\n",
    "#benchmarkFlash(kernels.benchmarks.loadFlash(big_randomQ, big_randomK, big_randomV, 32, 32, 8, 1280))\n",
    "\n",
    "vect, O = kernels.flashattn32_128.flashAttention(big_randomQ, big_randomK, big_randomV, 12, 1,  8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash_attn, time, math\n",
    "\n",
    "b, h = 1, 1\n",
    "qb = 4096\n",
    "xb = 9060*8\n",
    "d = 128\n",
    "\n",
    "Qs = big_randomQ.reshape(b, qb, h, d)\n",
    "Ks = big_randomK.reshape(b, xb, h, d)\n",
    "Vs = big_randomV.reshape(b, xb, h, d)\n",
    "\n",
    "# Qs = torch.rand(b, qbar, h, DIM, device='cuda', dtype=torch.float16)\n",
    "# Ks = torch.rand(b, xbar, h, DIM, device='cuda', dtype=torch.float16)\n",
    "# Vs = torch.rand(b, xbar, h, DIM, device='cuda', dtype=torch.float16)\n",
    "\n",
    "dsize = 2*(2*Qs.numel()+Ks.numel()+Vs.numel())\n",
    "print(Qs.shape)\n",
    "reps = 100\n",
    "# the average time\n",
    "t1 = 0\n",
    "# the second moment of time\n",
    "t2 = 0\n",
    "\n",
    "# the average throughput\n",
    "T1 = 0\n",
    "# the second moment of throughput\n",
    "T2 = 0\n",
    "\n",
    "for n in range(reps):\n",
    "    s0 = time.time()\n",
    "    Os = flash_attn.flash_attn_func(Qs, Ks, Vs)\n",
    "    torch.cuda.synchronize()\n",
    "    # Get average time\n",
    "    s1 = time.time()\n",
    "    dt = s1 - s0\n",
    "    t1 = (n*t1 + dt) / (n+1)\n",
    "    t2 = (n*t2 + dt**2) / (n+1)\n",
    "    # Get average throughput\n",
    "    Tn = (dsize / dt)/(1024**3)\n",
    "    T1 = (n*T1 + Tn) / (n+1)\n",
    "    T2 = (n*T2 + Tn**2) / (n+1) \n",
    "    if n > 10:\n",
    "        sd = math.sqrt((t2-t1**2)/(1-1/(n+1)))\n",
    "        if sd < t1/3:\n",
    "            break\n",
    "\n",
    "print(Os.shape)\n",
    "sd = math.sqrt((t2-t1**2)/(1-1/reps))\n",
    "Tsd = math.sqrt((T2-T1**2)/(1-1/reps))\n",
    "T = (dsize / t1)/(1024**3)\n",
    "print(f\"Throughput: {T:.2e} GiB/s ({(n+1)*t1:.2e}s, {t1:.2e}~{sd:.2e}s, {dsize:.2e}B)\")\n",
    "print(f\"Throughput: {T1:.2e} GiB/s ({(n+1)*t1:.2e}s, {T1:.2e}~{Tsd:.2e}s, {dsize:.2e}B)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Os\n",
    "(O-Os).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_randomA = torch.rand(112,96).cuda()\n",
    "big_randomB = torch.rand(112,96).cuda()\n",
    "# The second output are the number of us the function kernel takes.\n",
    "# kernels.benchmarks.matmul(big_randomA, big_randomB)\n",
    "\n",
    "# The speed for broadcasting data, hence, using L2 cache.\n",
    "# Scales with number of blocks (expected)\n",
    "# Reaches ~1.90 TB/s with all blocks receiving (32 warps, 120)\n",
    "def displayBenchmark(input : torch.Tensor):\n",
    "    t = input[0].item()\n",
    "    d = input[1].item()\n",
    "    w = input[2].item()\n",
    "    b = input[3].item()\n",
    "    r = input[4].item()\n",
    "    # Read Rate\n",
    "    T0 = ((d*r) / (t/1000))/(1024**3)\n",
    "    # Deposit Rate\n",
    "    T = (b*d*r / (t/1000))/(1024**3)\n",
    "    print(f\"Throughput: {T0:.2e} GiB/s in -> {T:.2e} GiB/s out ({t/1000:.2e}s,{d/1024:.0f}KiB,{w:.0f}w,{b:.0f}bl,{r:.1e}reps)\")\n",
    "    return T0, T\n",
    "if False:\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 8000000, 4, 1))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 8000000, 8, 1))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 8000000, 16, 1))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 8000000, 32, 1))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 800000, 16, 30))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 800000, 32, 30))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 800000, 32, 60))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 800000, 32, 120))\n",
    "    displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 800000, 32, 240))\n",
    "\n",
    "#displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 10000000, 16, 120))\n",
    "#displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 10000000, 16, 240))\n",
    "#displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 10000000, 32, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 10000, 32, 24000))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 10000, 16, 24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_randomA = torch.rand(128,96).cuda()\n",
    "#displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 10000, 32, 24000))\n",
    "print(\"Load GOOD\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 32, 32, 24000, 1))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 24, 32, 24000, 1))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 16, 32, 24000, 1))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 12, 32, 24000, 1))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 32,  24000, 1))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 32,  24000, 1))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 32,  24000, 1))\n",
    "print(\"Load ANDSAVE\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 32, 32, 24000, 4))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 24, 32, 24000, 4))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 16, 32, 24000, 4))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 12, 32, 24000, 4))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 32,  24000, 4))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 32,  24000, 4))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 32,  24000, 4))\n",
    "print(\"Load PIPELINE\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 32, 32, 24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 24, 32, 24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 16, 32, 24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 12, 32, 24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 32,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 32,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 32,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 1, 32,  24000, 5))\n",
    "print(\"Load BAD\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 32, 32, 24000, 2))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 24, 32, 24000, 2))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 16, 32, 24000, 2))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 12, 32, 24000, 2))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 32,  24000, 2))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 32,  24000, 2))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 32,  24000, 2))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 1, 32,  24000, 2))\n",
    "print(\"Warp Size 16\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 16, 16,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 16,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 16,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 16,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 1, 16,  24000, 5))\n",
    "mode = 5\n",
    "print(\"Warp Size 12\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 12, 12,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 10, 12,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 12,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 6, 12,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 12,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 12,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 1, 12,  24000,  mode))\n",
    "print(\"Smaller Warp Sizes\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 8,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 8,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 4,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 4,  24000, 5))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 2,  24000, 5))\n",
    "\n",
    "print(\"Load NONE\")\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 32, 32, 24000, 3))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 24, 32, 24000, 3))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 16, 32, 24000, 3))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 12, 32, 24000, 3))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 8, 32,  24000, 3))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 4, 32,  24000, 3))\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 1000, 2, 32,  24000, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_randomA = torch.rand(48,128).cuda()\n",
    "#displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 10000, 32, 24000))\n",
    "mode = 2\n",
    "print(\"Load PIPELINE\")\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 32, 32, 24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 24, 32, 24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 16, 32, 24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 12, 32, 24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 8, 32,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 4, 32,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 2, 32,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 1, 32,  24000, mode))\n",
    "print(\"Warp Size 16\")\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 16, 16, 24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 8, 16,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 4, 16,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 2, 16,  24000, mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 1, 16,  24000, mode))\n",
    "print(\"Warp Size 12\")\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 100000, 12, 12, 2400,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 100000, 10, 12, 2400,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 100000, 8, 12,  2400,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 100000, 6, 12,  2400,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 100000, 4, 12,  2400,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 100000, 2, 12,  2400,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 100000, 1, 12,  2400,  mode))\n",
    "print(\"Smaller Warp Sizes\")\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 8, 8,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 4, 8,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 4, 4,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 2, 4,  24000,  mode))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 1000, 2, 2,  24000,  mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 10000, 16, 32, 24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash_attn, time, math\n",
    "\n",
    "b, h = 1, 1\n",
    "qb = 1024\n",
    "xb = 9060*8\n",
    "d = 128\n",
    "\n",
    "Qs = torch.rand(b, qb, h, d, device='cuda', dtype=torch.float16)\n",
    "Ks = torch.rand(b, xb, h, d, device='cuda', dtype=torch.float16)\n",
    "Vs = torch.rand(b, xb, h, d, device='cuda', dtype=torch.float16)\n",
    "\n",
    "dsize = 2*(Qs.numel()+Ks.numel()+Vs.numel())\n",
    "print(Qs.shape)\n",
    "reps = 1000\n",
    "# the average time\n",
    "t1 = 0\n",
    "# the second moment of time\n",
    "t2 = 0\n",
    "\n",
    "for n in range(reps):\n",
    "    s0 = time.time()\n",
    "    Os = flash_attn.flash_attn_func(Qs, Ks, Vs)\n",
    "    torch.cuda.synchronize()\n",
    "    s1 = time.time()\n",
    "    dt = s1 - s0\n",
    "    t1 = (n*t1 + dt) / (n+1)\n",
    "    t2 = (n*t2 + dt**2) / (n+1)\n",
    "    if n > 10:\n",
    "        sd = math.sqrt((t2-t1**2)/(1-1/(n+1)))\n",
    "        if sd < t1/3:\n",
    "            break\n",
    "\n",
    "print(Os.shape)\n",
    "sd = math.sqrt((t2-t1**2)/(1-1/reps))\n",
    "T = (dsize / t1)/(1024**3)\n",
    "print(f\"Throughput: {T:.2e} GiB/s ({(n+1)*t1:.2e}s, {t1:.2e}~{sd:.2e}s, {dsize:.2e}B)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 4, 1))\n",
    "# displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 8, 1))\n",
    "# displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 16, 1))\n",
    "# displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 32, 1))\n",
    "# displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 32, 2))\n",
    "# displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 32, 4))\n",
    "# displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 32, 8))\n",
    "# Undersaturated - Prediction: Slower\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 32, 30))\n",
    "# Saturated - Prediction: Fastest\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 32, 60))\n",
    "# Oversaturated - Prediction: Equally Fastest\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 32, 120))\n",
    "# Undertimed - Prediction: Equally Fastest\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 8000, 32, 120))\n",
    "# Overtimed - Prediction: Equally Fastest\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 800000, 32, 120))\n",
    "# Undersaturated - Prediction: Slower (Warp Dependent, SM Dependent)\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 16, 60))\n",
    "# Saturated - Prediction: Slower (Warp Dependent), Equally Fastest (SM dependent)\n",
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 80000, 16, 120))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBenchmark(kernels.benchmarks.loadDistributed(big_randomA, 60, 32, 9600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 800000, 16, 360))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 20000000, 16, 240))\n",
    "# displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 20000000, 8, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_randomC = torch.rand(32,32).cuda()\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomC, 2000000, 4, 960))\n",
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomC, 2000000, 2, 1920))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "bs = [240,480,720,960,1200,1440,1680,1920,2160,2400]\n",
    "reps = 2000000000\n",
    "for b in bs:\n",
    "   times.append(displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomC, reps // b, 4, b)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(bs, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbm, hbm_size, niter = kernels.benchmarks.testCopy(True)\n",
    "l2, l2_size, niter = kernels.benchmarks.testCopy(False)\n",
    "\n",
    "# hbm_through = 2*niter*(4*hbm_size)/(hbm/1000)\n",
    "# l2_through = 2*niter*(4*l2_size)/(l2/1000)\n",
    "print(hbm, hbm_size, niter)\n",
    "print(l2, l2_size, niter)\n",
    "\n",
    "print(f\"{hbm_through:0.2e}\")\n",
    "print(f\"{l2_through:0.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayBenchmark(kernels.benchmarks.loadBroadcasted(big_randomA, 4000000, 32, 120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
