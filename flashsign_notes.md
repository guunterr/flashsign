## GOALS

- Figure out the neural string diagrams
    - Work through the paper and repeat calculations and diagrams
    - Understand what the fuck he means by indexing
    - Reprove the theorems
    - Figure out how to replicate the performance model calculations
    - Try to replicate SGAttention optimisations without just copying his homework
    - Maybe implement some automatic constraint optimisation stuff for architectural search
- Learn how to write CUDA and get stuff working
    - Implement decent saxpy or GEMM
        - Speed this up to somewhat near cublas rate
- Get a GPU on Vast.ai
    - Find an H100 I can rent
    - Set up an instance to do my programming
    - Set up infrastructure for running tests and benchmarks
    - Set up infrastructure for hyperparameter search on algorithms
- FlashAttention
    - Set up a git repo
    - Iteratively work up to implementing what's in the papers vincent has sent me
- SGAttention
    - Write out vincent's naive diagram into cuda
    - Write out better diagrams into cuda
    - Benchmark all of these for the paper
    - Iteratively improve algorithm until we are >10x faster than pytorch

- SGEMM Kernels
    - Kernel 1 60 GFLOPS (BAD!)
    - Kernel 2 231 GFLOPS
    - Kernel 3 260 GFLOPS
    - Kernel 4 on the way